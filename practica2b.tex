%Encabezado estándar
\documentclass[10pt,a4paper]{article}
\usepackage{pgfplots}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure} %paquete para poder añadir subfiguras a una figura
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage[toc,page]{appendix} %paquete para hacer apéndices
\usepackage{cite} %paquete para que Latex contraiga las referencias [1-4] en lugar de [1][2][3][4]
\usepackage[nonumberlist]{glossaries} %[toc,style=altlistgroup,hyperfirst=false] 
%usar makeglossaries grafo para recompilar el archivo donde están los grafos y que así salga actualizado
\author{Gustavo Rivas Gervilla DNI: 75570417F \\ gustavofox92@correo.ugr.es \\5º Doble Grado en Ing. Informática y Matemáticas \\Grupo 3}
\title{Práctica 2.b: Búsquedas Multiarranque para el Problema de la Selección de Características \\ BMB \\ GRASP \\ ILS}
\date{}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.55in}
%Configuración especial
\setlength{\parindent}{0cm}
\pretolerance=10000
\tolerance=10000
\renewcommand{\contentsname}{\color[rgb]{0.0,0.0,0.21}Índice}
\renewcommand{\tablename}{\color[rgb]{0.5,0.0,0.0}Tabla}

\hypersetup{
  colorlinks=true,%colorear el texto en lugar de poner una caja de color alrededor
  citecolor=orange,%citas bibliográficas, del estilo [8]
  urlcolor=orange,%urls
  linkcolor=[rgb]{0.0,0.0,0.21}}%links internos como los del índice
  
\lstdefinestyle{customPy}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=Python,
  showstringspaces=true,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}

\begin{document}
\lstset{language=Python, style=customPy}
\maketitle

\newpage

\tableofcontents

\newpage

\section{\color[rgb]{0.0,0.0,0.21}Descripción/formulación del problema abordado}
Lo que intentamos hacer con nuestros algoritmos es encontrar un conjunto de características de unos datos que nos permitan realizar una clasificación suficientemente buena de nuevos datos que nos lleguen con las mismas características.\\

Hay un problema muy habitual en la vida real y es la de clasificar una serie de elementos en distintas categorías en función de información sobre ellos, esta tarea puede ser realizada por personas o, lo que es más eficiente, por un ordenador. Para realizar tal clasificación es habitual que se recojan multitud de datos sobre los distintos elementos que se quieren clasificar, de modo que en base a esta información podamos decidir si el elemento es de una categoría o de otra. Pensemos por ejemplo en clasificar fruta en base a si se desecha o no. Podemos pensar en recoger datos sobre el tamaño de esa fruta, su color, su textura o su dureza y en base a estas mediciones una máquina debería clasificar la fruta en buena o mala.\\

El problema está en que normalmente no se conoce tan bien el campo de estudio como para saber a ciencia cierta qué datos recoger, qué datos serán más relevantes a la hora de clasificar elementos de una determinada población. Entonces lo que se hace es recoger gran cantidad de información sobre los elementos para al menos intentar que no haya carencias en la información, esto por supuesto conlleva tanto el coste de adquirir esa información (no sabemos cómo de caro es realizar una determinada medición) como el coste computacional de procesar toda esa información. Entonces lo que nos gustaría es averiguar qué información, de entre toda la que hemos obtenido, es la verdaderamente relevante para la clasificación que queremos realizar.\\

Entonces partiendo de un conjunto de datos de aprendizaje, valores de características de distintos elementos, queremos ver con qué subconjunto de características podemos hacer una buena clasificación de esos elementos, así si tenemos que cada dato viene dado por una lista de n características $[f_1, f_2, ..., f_n]$ queremos obtener un subconjunto de esas características, de modo que teniendo sólo la información $[f_{s1}, ..., f_{sm}]$, se haga una buena clasificación del conjunto de datos de aprendizaje, del que conocemos por supuesto la clasificación perfecta de dichos datos. Y esperamos que con esa misma información se clasifiquen lo mejor posible nuevos elementos de fuera de la muestra de aprendizaje.\\

\newpage

\section{\color[rgb]{0.0,0.0,0.21}Descripción de la aplicación de los algoritmos empleados al problema}

Dado que estamos ante un problema de selección las soluciones se representarán como vectores binarios de booleanos de tamaño el número de características a elegir, indicando si una característica se considera o no, así tendremos claramente un espacio de búsqueda de $2^n$, siendo $n$ el número de características a elegir que también lo podemos ver como el tamaño del problema abordado.\\

Entonces para evaluar como de buena es una determinada solución hacemos lo siguiente:\\

\begin{lstlisting}
tomamos de cada dato de entrenamiento las caracteristicas seleccionada por la solucion.

for cada dato de entrenamiento:
	clasificador_dato = construir clasificador 3NN con el resto de datos
	ver si clasificador_dato clasifica correctamente a ese dato
	
return porcentaje de aciertos
\end{lstlisting}

Nuestros algoritmos irán explorando, según su mecanismo, el espacio de soluciones empleando la función anterior para tomar decisiones sobre qué movimientos se realizan en dicho espacio de búsqueda. Para poder generar las soluciones vecinas a una dada empleamos el operador de Flip, el cual funciona del siguiente modo:\\

\begin{lstlisting}
def flip(sol, idx):
	cambiar el valor de la pos. idx de la sol por su negado
\end{lstlisting}

Cuando finalice su proceso de búsqueda los algoritmos nos devolverán la solución que ellos han elegido (junto con el score dentro de la muestra de entrenamiento calculada como hemos dicho antes). Entonces una vez tenemos la solución nos queda por evaluar cómo de bien se clasifican los datos empleando un clasificador 3NN construido sólo con aquellas características seleccionadas por nuestra solución:\\

\begin{lstlisting}
claficador = construir clasificador con los datos de entrenamiento solo con las caracterisiticas seleccionadas por nuestra solucion
return el porcentaje de acierto de este clasificador etiquetando los datos de test de los que consideramos solo las caracteristicas seleccionadas por la solucion
\end{lstlisting}

A cada algoritmo le daremos solamente los datos de entrenamiento separados en características y etiquetas pera que con la estragia de búsqueda que implemente nos devuelva la mejor solución posible para él, luego la evaluación de la solución final la haremos fuera del algoritmo con los datos de test (en la sección 8 explicaremos cómo hemos generado las particiones). Para aquellos algoritmos que empleando algún tipo de aleatoriedad en sus decisiones inicilizaremos la semilla aleatoria antes de la ejecución de dicho algoritmo.\\

Tanto en la BMB como en el ILS hemos de generar unas soluciones aleatorias, para cada algoritmo esta solución aleatoria se generada de un modo diferente:\\

Para la BMB:\\

\begin{lstlisting}
for cada iteracion del algoritmo:
	Obtengo una muestra aleatoria de len = n_caracteristica muestreada uniforme y con remplazamiento.
\end{lstlisting}

Para el ILS:\\

\begin{lstlisting}
al inicio del algoritmo y nunca mas:
	Obtengo una muestra aleatoria de len = n_caracteristica muestreada uniforme y con remplazamiento.
\end{lstlisting}

El proceso de búsqueda local empleado es el que implementamos en la primera práctica, como sabemos la factorización en el caso de la selección de características no es trivial por lo que en nuestro caso, al contrario que en el QAP, no empleamos ningún mecanismo de factorización para el cálculo de la función objetivo. El operador de generación de vecino ya ha sido descrito anteriormente, \textit{flip}. El mecanismo de búsqueda de la búsqueda local es como sigue:\\

\begin{lstlisting}
s = solucion que le pasamos por param.
n_sols_generados = 0

while n_sols_generados < max_permitido:
	for cada vecino de s explorados en orden aleatorio:
		n_sols_generados++
		
		if el vecino es mejor que s:
			s = el vecino
			dejamos de explorar vecinos
					
	if no hemos encontrado ninguna solucion mejor en todo el vecindario:
		return s
		
return s
\end{lstlisting}

Aquí hay un cambio fundamental con respecto a la práctica anterior y es que antes la solución de la que se partía era aleatoria, ahora nosotros le decimos con qué solución comenzar el proceso de búsqueda local, que puede ser con una solución aleatoria como en el caso de la BMB o con una mutación de la mejor solución encontrada hasta el momento como en el ILS.\\

\newpage

\section{\color[rgb]{0.0,0.0,0.21}Descripción en pseudocódigo de los algoritmos}
\subsection{\color[rgb]{0.0,0.0,0.51}Búsqueda Multiarranque Básica (BMB.py)}

\begin{lstlisting}
for 25 veces:
	s_r = sol. aleatoria
	s = sol. encontrada por las Busqueda Local partiendo desde s_r
	
	if s es mejor que la mejor sol. encontrada hasta el momento:
		mejor_s = s
		
return mejor_s
\end{lstlisting}

\newpage

\subsection{\color[rgb]{0.0,0.0,0.51}GRASP (GRASP.py)}

El algoritmo de generación de soluciones greedy probabilísticas funciona del siguiente modo:\\
\begin{lstlisting}
sol = un array binario con todo False #conjunto vacio de caracteristicas

while tengamos ganancia and queden caracteristicas por aniadir:
	calcular el score de cada caracteristica por aniadir al agregarla al conjunto actual
	
	mu = mejor_score_de_los_candidatos - 0.3(mejor_score_de_los_candidatos - mejor_score_de_los_candidatos)
	
	lista_restringida_candidatos = caracteristicas de las anteriores que den ganancia >= mu
	elegimos una caracteristica aleatoria de la listra restringida
	
	if el score aniadiendola es mejor que el de el conjunto mejor al que habia:
		se agrega dicha caracteristica al conjunto
		quitamos esa caracteristica de el conjunto de caracteristicas por aniadir
	else:
		no tenemos ganancia y acabamos el bucle
		
return el conjunto de caracteristicas al que hemos llegado
\end{lstlisting}

\begin{lstlisting}
for 25 veces:
	s_greedy = sol. encontrada por el greedy aleatorio
	s = sol. encontrada por la Busquda Local partiendo desde s_greedy
	
	if s es mejor que la mejor sol. encontrada hasta el momento:
		mejor_s = s
		
return mejor_s	
\end{lstlisting}

\newpage
\subsection{\color[rgb]{0.0,0.0,0.51} ILS (ILS.py)}
\begin{lstlisting}
mutar(s):
	t = ceil(0.1*n_caracteristicas)
	genes_a_mutar = elegimos t genes aleatorios, sin repeticion, para ser mutados
	cambiamos el valor de cada gen de s a mutar por su complementario
	
\end{lstlisting}

\begin{lstlisting}
s = sol. inicial aleatoria
s = sol. encontrada por Busqueda Local partiendo de s

for 24 veces:
	if s es mejor que la mejor sol. encontrada:
		best_s = s
		
	s = mutar(best_s)
	s = sol. encontrada por Busqueda Local partiendo de s
	
return best_s
\end{lstlisting}
\newpage

\section{\color[rgb]{0.0,0.0,0.21}Breve descripción del algoritmo de comparación}

Para el algoritmo de comparación, SFS, lo que hacemos es lo que vamos a reflejar en el siguiente pseudocódigo, la principal diferencia es que no tenemos la función usual flip sino que hemos hecho una función especial para poder realizar una función vectorizada en Python (aunque no produce ninguna mejora en tiempo a la versión que tendríamos si simplemente tuviésemos un for que recorriese las características que quedan por añadir hasta encontra la de mayor ganancia), esta función crea una nueva solución poniendo a True una componente de la solución que le pasamos y nos da su porcentaje de clasificación, como hacemos en el resto de algoritmos. Dicho esto pasamos al pseudocódigo:

\begin{lstlisting}
sol = un array binario con todo False #conjunto vacio de caracteristicas

while tengamos ganancia and queden caracteristicas por aniadir:
	calcular el score de cada caracteristica por aniadir al agregarla al conjunto actual
	tomar la caracteristica que de mejor score en el calculo anterior
	
	if el score aniadiendola es mejor que el de el conjunto mejor al que habia:
		se agrega dicha caracteristica al conjunto
		quitamos esa caracteristica de el conjunto de caracteristicas por aniadir
	else:
		no tenemos ganancia y acabamos el bucle
		
return el conjunto de caracteristicas al que hemos llegado
\end{lstlisting}
\newpage

\section{\color[rgb]{0.0,0.0,0.21}Procedimiento considerado para desarrollar la práctica}

\subsection{\color[rgb]{0.0,0.0,0.51}Desarrollo}

El código usado en prácticas lo he implementado yo a partir de las explicaciones dadas tanto en clase de prácticas como de teoría sobre los distintos algoritmos, siendo de mucha utilidad las indicaciones del gruión de práctias.\\

La parte que no ha sido implementada por mí es la correspondiente a la evaluación de las soluciones, es decir, tanto el KNN como los mecanismos de evaluación de las soluciones. Para ello he usado el código desarrollado por \textbf{Alejandro García Montoro} el cuál ha implementado tanto el clasificador KNN como los mecanismos para obtener el score de las soluciones dentro (con LOO) y fuera de la muestra con PyCUDA que permite insertar código CUDA en código Python. Con lo cual los tiempos de ejecución bajan considerablemente con respecto a los tiempos de la práctica anterior ya que el cuello de botella de los algoritmos, que es la evaluación de las soluciones con LOO, se ejecuta en la tarjeta gráfica, siendo mucho más rápidos los cálculos.\\

\subsection{\color[rgb]{0.0,0.0,0.51}Manual de usuario}
Los algoritmos han sido implementados en Python 3.1 TODO para su ejecución son necesarios tener instalados los módulos compatibles con esta versión de Python siguientes:\\

\begin{itemize}
\item numpy (v 1.8.2) \textbf{sudo apt-get install python-numpy}
\item scikit \textbf{sudo apt-get install python-scikits-learn}
\item jinja2
\item pycuda
\item El resto de módulos empleados suelen venir con las distribuciones básicas de Python (random, sys y time).
\end{itemize}

Cada uno de los algoritmos están implementados en distintos ficheros de los que podemos ver su contenido en el \textbf{\textcolor{green}{LEEME}} que se nos pide que incluyamos con el código. Los experimentos han sido ejecutados usando el fichero main al que le pasamos por argumentos el nombre del algoritmo a usar: 3NN, SFS, BMB, GRASP e ILS. La semilla se establece dentro del fichero al inicio de la ejecución del algoritmo para cada una de las particiones. Esto lo hemos hecho así para poder cortar la obtención de datos cuando sea necesaria y poder retomar los experimento desde la partición por las que nos quedáramos, así no se desajusta la semilla con la que hacemos los experimentos al retomar.\\

Entonces para ejecutar por ejemplo los experimentos para el GRASP lo que haremos es \textit{python mainCUDA.py GRASP} y se ejecutará el algoritmo sobre todas las particiones de datos que tenemos.\\

En el directorio \textbf{\textcolor{green}{make\_partitions32b}} tenemos tanto los archivos arff como los códigos necesarios para elaborar las particiones que serán utilizadas por nuestros algoritmos. Esto no es necesario para ejecutar los programas puesto que en el directorio \textbf{\textcolor{green}{partitions32b}} tenemos almacenadas todas las particiones construidas. No obstante si se quisiera replicar el procedimiento de desarrollo de particiones lo que haríamos sería: eliminar los archivos con extensión .npy del directorio \textbf{make\_partitions32b} (para tener seguridad de que no se corrompan dichos archivos), ejecutar el loader con la instrucción \textbf{python loader.py} y a continuación elaborar las particiones con \textbf{python partition\_maker.py} (dentro de este directorio que es donde se encuentran ambos códigos). Esto generará diversos archivos de extensión .npy con nombres que empiezan por arr, wdbc o libras (según la base) seguida de un número y la palabra test o training.\\

Estos archivos deberán reemplazar a aquellos que se encuentran en el directorio \textbf{partitions32b} y ya podremos ejecutar los algoritmos con dichas particiones desde el main.\\

\newpage
\section{\color[rgb]{0.0,0.0,0.21}Experimentos y análisis de resultados}

\subsection{\color[rgb]{0.0,0.0,0.51}Descripción de los casos del problema empleados y de los valores de los parámetros}


Estás prácticas han sido implementadas en Python 3.5.1 y ejecutadas sobre un ordenador son S.O. Arch Linux, de 12GB de RAM y procesador Intel Core i7 930 2.80GHz y tarjeta gráfica NVIDIA GeForce GTX 780. Este ordenador pertenece a Alejandro Garcia Montoro ya que tuve problemas al intentar usar los drivers oficiales de Nvidia en Ubuntu (necesarios para ejecutar código CUDA) y me ha permitido lanzar mis códigos en su ordenador mediante conexión ssh.\\

Hemos empleado tres bases de datos distintas, a continuación mencionamos sus tamaños ya que serán interesantes para el posterior análisis de resultados:\\

\begin{itemize}
\item \textbf{wdbc}: Tenemos \textbf{569} muestras con \textbf{30} características cada una procedentes de imágenes digitalizadas de una masa de mama. Estas muestras se clasifican en \textbf{2} clases distintas.
\item \textbf{libras}: Aquí tenemos \textbf{360} muestras de \textbf{90} características cada una, éstas muestras son datos de distintos movimientos de la mano que se clasifican en \textbf{15} clases distintas.
\item \textbf{arritmia}: Los datos de este conjunto son mediciones para determinar la presencia de arritmia cardiaca o no. Tenemos \textbf{386} muestras con \textbf{278} caracterísitcas cada una, a clasificar en \textbf{5} grupos en base al tipo de arritmia que indican los datos de la muestra.
\end{itemize}

Lo que hemos hecho con los datos ha sido un preprocesado, en primer lugar todos los datos han sido codificados como flotantes (las etiquetas de wdbc eran cadenas de texto y las hemos cambiado por los valores 0 y 1), también hemos puesto la etiqueta en la última columna de la tabla de datos. Es importante señalar que para poder emplear el KNN implementado en pyCUDA que hemos mencionado antes hemos tenido que codificar los distintos datos de las particiones en 32 bits ya que así es como estaba configurado el clasificador y CUDA no hace correctamente el casting de 64b a 32b.\\

También hemos normalizado cada una de las columnas de datos (sin contar la de etiquetas) de modo que los valores quedaran en el intervalo [0,1] mediantes la fórmula:\\

\begin{center}
$x_j^N = \dfrac{x_j - Min_j}{Max_j-Min_j}$ (siendo $Max_j$ y $Min_j$ el máximo y mínimo valor de los datos para la característica j-ésima de las muestras).
\end{center}

El código empleado para este formateo de los datos está en el fichero \textbf{\textcolor{green}{loader.py}} que se adjunta en la entrega. Como podemos ver para realizar la normalización de los datos hemos usado una utilidad del módulo scikit-learn antes mencionado que se llama MinMaxScaler. Y para futuros usos de estos datos normalizados, y con el objetivo de no tener que realizar tales operaciones cada ocasión que queramos usarlos, hemos almacenado los arrays de numpy donde hemos almacenado los datos en sendos ficheros con extensión .npy usando la función \textbf{numpy.save}. Estos ficheros son: \textbf{\textcolor{green}{data\_wdbc.npy}}, \textbf{\textcolor{green}{data\_libras.npy}} y \textbf{\textcolor{green}{data\_arrhythmia.npy}}.\\

A continuación hemos elaborado las distintas particiones sobre estos datos que serán utilizadas en los experimentos, para ello hemos elaborado el código recogido en el fichero \textbf{\textcolor{green}{partition\_maker.npy}} donde simplemente tomamos una muestra aleatoria de los índices de los arrays antes generados, teniendo en cuenta que la cantidad de muestras de cada una de las clases en las que se clasifican los datos sea lo más equilibrida posible entre la partición de test y su correspondiente partición de training. La semilla usada para la generación de dichas particiones, y que podemos ver en el fichero mencionado, ha sido la \textbf{12345678}. Nuevamente hemos almacenado cada una de dichas particiones en ficheros .npy, los cuáles están en la carpeta \textbf{\textcolor{green}{partitions32b}}.\\

Los parámetros empleados en cada uno de los algoritmos son los que se indican en el guión de prácticas. Para cada una de las ejecuciones de lo algoritmos inicializamos la semilla aleatoria a \textbf{12345678} nuevamente, esto lo podemos ver en el fichero \textbf{\textcolor{green}{mainCUDA.py}}.\\


\subsection{\color[rgb]{0.0,0.0,0.51}Resultados}

Adjuntamos también las tablas en un fichero por si no se leen correctamente en el pdf.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{3NN}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
\multirow{2}{*}{}                   & \multicolumn{4}{c|}{Wdbc}                     & \multicolumn{4}{c|}{Movement\_Libras}         & \multicolumn{4}{c|}{Arrhythmia}               \\ \cline{2-13} 
                                    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    \\ \hline
\multicolumn{1}{|l|}{Partición 1-1} & 96,14        & 96,13         & 0,00    & 0,00 & 75,56        & 76,11         & 0,00    & 0,00 & 65,46        & 64,06         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 1-2} & 96,83        & 95,79         & 0,00    & 0,00 & 67,78        & 70,56         & 0,00    & 0,00 & 59,38        & 62,37         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 2-1} & 95,44        & 96,47         & 0,00    & 0,00 & 70,00        & 67,78         & 0,00    & 0,00 & 62,89        & 62,50         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 2-2} & 96,13        & 96,49         & 0,00    & 0,00 & 74,44        & 72,78         & 0,00    & 0,00 & 63,02        & 64,43         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 3-1} & 93,68        & 97,18         & 0,00    & 0,00 & 70,56        & 73,33         & 0,00    & 0,00 & 60,82        & 65,10         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 3-2} & 97,54        & 95,44         & 0,00    & 0,00 & 70,00        & 72,22         & 0,00    & 0,00 & 61,98        & 64,95         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 4-1} & 96,14        & 95,42         & 0,00    & 0,00 & 72,22        & 72,22         & 0,00    & 0,00 & 60,82        & 64,58         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 4-2} & 98,24        & 95,44         & 0,00    & 0,00 & 70,00        & 74,44         & 0,00    & 0,00 & 64,58        & 62,37         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 5-1} & 96,14        & 95,42         & 0,00    & 0,00 & 71,11        & 78,33         & 0,00    & 0,00 & 67,01        & 63,02         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Partición 5-2} & 97,89        & 95,44         & 0,00    & 0,00 & 71,67        & 73,33         & 0,00    & 0,00 & 63,54        & 62,37         & 0,00    & 0,00 \\ \hline
\multicolumn{1}{|l|}{Media}         & 96,42        & 95,92         & 0,00    & 0,00 & 71,33        & 73,11         & 0,00    & 0,00 & 62,95        & 63,58         & 0,00    & 0,00 \\ \hline
\end{tabular}}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{SFS}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
\multirow{2}{*}{}                   & \multicolumn{4}{c|}{Wdbc}                     & \multicolumn{4}{c|}{Movement\_Libras}         & \multicolumn{4}{c|}{Arrhythmia}               \\ \cline{2-13} 
                                    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    \\ \hline
\multicolumn{1}{|l|}{Partición 1-1} & 95,44        & 95,42         & 83,33   & 0,24 & 78,89        & 72,22         & 90,00   & 1,04 & 79,90        & 68,23         & 97,12   & 3,00 \\ \hline
\multicolumn{1}{|l|}{Partición 1-2} & 97,54        & 92,98         & 90,00   & 0,15 & 74,44        & 60,00         & 93,33   & 0,69 & 74,48        & 70,10         & 98,56   & 1,51 \\ \hline
\multicolumn{1}{|l|}{Partición 2-1} & 97,19        & 97,54         & 86,67   & 0,20 & 77,22        & 66,11         & 88,89   & 1,16 & 84,54        & 71,35         & 97,12   & 3,00 \\ \hline
\multicolumn{1}{|l|}{Partición 2-2} & 97,54        & 95,79         & 90,00   & 0,15 & 76,11        & 65,56         & 91,11   & 0,92 & 72,92        & 69,59         & 99,28   & 0,85 \\ \hline
\multicolumn{1}{|l|}{Partición 3-1} & 96,49        & 95,77         & 83,33   & 0,24 & 78,33        & 73,89         & 90,00   & 1,04 & 77,32        & 69,79         & 97,48   & 2,62 \\ \hline
\multicolumn{1}{|l|}{Partición 3-2} & 97,89        & 95,09         & 86,67   & 0,20 & 76,11        & 75,00         & 91,11   & 0,92 & 85,94        & 74,23         & 96,04   & 4,27 \\ \hline
\multicolumn{1}{|l|}{Partición 4-1} & 95,09        & 91,55         & 93,33   & 0,11 & 76,11        & 67,22         & 91,11   & 0,92 & 79,90        & 67,19         & 96,04   & 4,24 \\ \hline
\multicolumn{1}{|l|}{Partición 4-2} & 98,24        & 94,04         & 76,67   & 0,33 & 70,56        & 65,56         & 93,33   & 0,69 & 83,33        & 78,87         & 96,76   & 3,41 \\ \hline
\multicolumn{1}{|l|}{Partición 5-1} & 96,84        & 97,18         & 86,67   & 0,20 & 68,33        & 62,78         & 93,33   & 0,70 & 84,02        & 69,79         & 96,04   & 4,26 \\ \hline
\multicolumn{1}{|l|}{Partición 5-2} & 98,24        & 94,04         & 83,33   & 0,24 & 74,44        & 66,11         & 92,22   & 0,81 & 83,85        & 73,71         & 96,04   & 4,25 \\ \hline
\multicolumn{1}{|l|}{Media}         & 97,05        & 94,94         & 86,00   & 0,21 & 75,05        & 67,45         & 91,44   & 0,89 & 80,62        & 71,29         & 97,05   & 3,14 \\ \hline
\end{tabular}}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{Resultados BMB}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
\multirow{2}{*}{}                   & \multicolumn{4}{c|}{Wdbc}                     & \multicolumn{4}{c|}{Movement\_Libras}          & \multicolumn{4}{c|}{Arrhythmia}                 \\ \cline{2-13} 
                                    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T     & \%\_clas\_in & \%\_clas\_out & \%\_red & T      \\ \hline
\multicolumn{1}{|l|}{Partición 1-1} & 98,25        & 95,42         & 40,00   & 3,60 & 79,44        & 72,78         & 51,11   & 12,98 & 74,23        & 65,10         & 49,64   & 125,10 \\ \hline
\multicolumn{1}{|l|}{Partición 1-2} & 98,59        & 95,44         & 43,33   & 3,48 & 77,78        & 69,44         & 54,44   & 13,65 & 71,88        & 65,98         & 50,00   & 143,26 \\ \hline
\multicolumn{1}{|l|}{Partición 2-1} & 97,89        & 95,77         & 56,66   & 3,70 & 77,22        & 67,22         & 62,22   & 12,95 & 74,23        & 64,06         & 47,84   & 141,67 \\ \hline
\multicolumn{1}{|l|}{Partición 2-2} & 98,59        & 95,09         & 56,66   & 3,28 & 78,33        & 70,00         & 57,78   & 12,29 & 70,31        & 64,95         & 53,96   & 144,62 \\ \hline
\multicolumn{1}{|l|}{Partición 3-1} & 96,84        & 96,83         & 46,66   & 3,47 & 77,78        & 71,67         & 51,11   & 12,09 & 72,16        & 64,58         & 53,60   & 121,83 \\ \hline
\multicolumn{1}{|l|}{Partición 3-2} & 98,94        & 95,09         & 43,33   & 3,04 & 76,67        & 71,67         & 54,44   & 14,37 & 72,92        & 63,92         & 53,60   & 164,83 \\ \hline
\multicolumn{1}{|l|}{Partición 4-1} & 98,25        & 95,42         & 50,00   & 3,68 & 78,89        & 75,00         & 48,89   & 12,08 & 72,68        & 67,71         & 52,88   & 152,11 \\ \hline
\multicolumn{1}{|l|}{Partición 4-2} & 98,94        & 95,09         & 50,00   & 3,41 & 76,67        & 77,22         & 54,44   & 12,25 & 75,00        & 64,43         & 54,68   & 154,49 \\ \hline
\multicolumn{1}{|l|}{Partición 5-1} & 98,25        & 95,42         & 43,33   & 3,18 & 75,00        & 77,78         & 56,67   & 14,36 & 73,71        & 66,15         & 51,08   & 123,77 \\ \hline
\multicolumn{1}{|l|}{Partición 5-2} & 98,94        & 94,74         & 53,33   & 3,37 & 78,89        & 77,22         & 61,11   & 13,00 & 72,40        & 61,86         & 50,00   & 140,37 \\ \hline
\multicolumn{1}{|l|}{Media}         & 98,35        & 95,43         & 48,33   & 3,42 & 77,67        & 73,00         & 55,22   & 13,00 & 72,95        & 64,87         & 51,73   & 141,21 \\ \hline
\end{tabular}}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{GRASP}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
\multirow{2}{*}{}                   & \multicolumn{4}{c|}{Wdbc}                      & \multicolumn{4}{c|}{Movement\_Libras}          & \multicolumn{4}{c|}{Arrhythmia}                 \\ \cline{2-13} 
                                    & \%\_clas\_in & \%\_clas\_out & \%\_red & T     & \%\_clas\_in & \%\_clas\_out & \%\_red & T     & \%\_clas\_in & \%\_clas\_out & \%\_red & T      \\ \hline
\multicolumn{1}{|l|}{Partición 1-1} & 98,6         & 91,90         & 80,00   & 9,51  & 81,67        & 75,56         & 86,67   & 33,30 & 85,05        & 72,92         & 93,88   & 132,67 \\ \hline
\multicolumn{1}{|l|}{Partición 1-2} & 98,94        & 95,44         & 46,67   & 10,88 & 78,33        & 72,22         & 85,56   & 36,15 & 82,29        & 67,01         & 92,81   & 99,77  \\ \hline
\multicolumn{1}{|l|}{Partición 2-1} & 97,89        & 94,01         & 66,67   & 8,38  & 78,89        & 67,22         & 86,67   & 34,03 & 84,54        & 77,08         & 94,60   & 118,87 \\ \hline
\multicolumn{1}{|l|}{Partición 2-2} & 98,59        & 96,14         & 23,33   & 10,54 & 81,67        & 71,11         & 84,44   & 36,00 & 82,29        & 73,71         & 93,88   & 78,29  \\ \hline
\multicolumn{1}{|l|}{Partición 3-1} & 97,19        & 96,83         & 60,00   & 9,11  & 80,00        & 70,00         & 87,78   & 35,56 & 82,47        & 70,83         & 92,45   & 98,61  \\ \hline
\multicolumn{1}{|l|}{Partición 3-2} & 98,94        & 96,14         & 70,00   & 11,25 & 78,89        & 75,00         & 84,44   & 32,54 & 84,38        & 70,62         & 93,17   & 157,76 \\ \hline
\multicolumn{1}{|l|}{Partición 4-1} & 97,89        & 95,42         & 60,00   & 9,18  & 83,33        & 72,78         & 77,78   & 38,11 & 81,44        & 73,96         & 92,45   & 102,19 \\ \hline
\multicolumn{1}{|l|}{Partición 4-2} & 98,94        & 94,74         & 46,67   & 9,68  & 80,56        & 71,67         & 80,00   & 35,71 & 85,94        & 73,71         & 90,29   & 143,39 \\ \hline
\multicolumn{1}{|l|}{Partición 5-1} & 98,25        & 91,90         & 70,00   & 10,13 & 77,22        & 78,89         & 86,67   & 31,41 & 86,08        & 69,79         & 94,96   & 108,93 \\ \hline
\multicolumn{1}{|l|}{Partición 5-2} & 98,94        & 94,39         & 73,33   & 10,98 & 81,67        & 75,00         & 81,11   & 32,40 & 82,81        & 76,29         & 93,17   & 99,15  \\ \hline
\multicolumn{1}{|l|}{Media}         & 98,42        & 94,69         & 59,67   & 9,96  & 80,22        & 72,95         & 84,11   & 34,52 & 83,73        & 72,59         & 93,17   & 113,96 \\ \hline
\end{tabular}}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{ILS}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
\multirow{2}{*}{}                   & \multicolumn{4}{c|}{Wdbc}                     & \multicolumn{4}{c|}{Movement\_Libras}          & \multicolumn{4}{c|}{Arrhythmia}                 \\ \cline{2-13} 
                                    & \%\_clas\_in & \%\_clas\_out & \%\_red & T    & \%\_clas\_in & \%\_clas\_out & \%\_red & T     & \%\_clas\_in & \%\_clas\_out & \%\_red & T      \\ \hline
\multicolumn{1}{|l|}{Partición 1-1} & 98,6         & 94,37         & 63,33   & 3,06 & 79,44        & 76,67         & 46,66   & 12,04 & 79,38        & 66,67         & 52,52   & 114,28 \\ \hline
\multicolumn{1}{|l|}{Partición 1-2} & 98,6         & 95,09         & 53,33   & 3,10 & 74,44        & 71,67         & 48,89   & 10,99 & 74,48        & 67,01         & 55,40   & 111,76 \\ \hline
\multicolumn{1}{|l|}{Partición 2-1} & 98,25        & 96,13         & 53,33   & 3,60 & 75,56        & 70,00         & 52,22   & 10,83 & 74,74        & 65,10         & 53,24   & 95,07  \\ \hline
\multicolumn{1}{|l|}{Partición 2-2} & 98,59        & 95,79         & 46,66   & 3,21 & 80,00        & 73,89         & 52,22   & 11,36 & 74,48        & 72,16         & 53,96   & 119,37 \\ \hline
\multicolumn{1}{|l|}{Partición 3-1} & 97,19        & 97,54         & 43,33   & 3,15 & 77,78        & 73,33         & 53,33   & 12,83 & 76,29        & 68,23         & 55,04   & 102,29 \\ \hline
\multicolumn{1}{|l|}{Partición 3-2} & 99,3         & 95,79         & 36,66   & 3,13 & 78,89        & 71,11         & 53,33   & 11,38 & 76,56        & 65,98         & 57,91   & 115,71 \\ \hline
\multicolumn{1}{|l|}{Partición 4-1} & 98,25        & 96,13         & 46,66   & 2,47 & 81,11        & 74,44         & 54,44   & 11,94 & 76,80        & 68,23         & 51,80   & 114,23 \\ \hline
\multicolumn{1}{|l|}{Partición 4-2} & 99,3         & 94,39         & 33,33   & 3,07 & 78,33        & 75,00         & 55,56   & 13,08 & 76,04        & 62,89         & 50,72   & 112,01 \\ \hline
\multicolumn{1}{|l|}{Partición 5-1} & 97,89        & 94,72         & 43,33   & 2,91 & 76,11        & 78,89         & 54,44   & 12,41 & 76,29        & 67,19         & 48,20   & 129,35 \\ \hline
\multicolumn{1}{|l|}{Partición 5-2} & 99,3         & 95,44         & 43,33   & 2,67 & 78,89        & 78,89         & 45,56   & 11,46 & 76,04        & 63,92         & 56,12   & 105,87 \\ \hline
\multicolumn{1}{|l|}{Media}         & 98,53        & 95,54         & 46,33   & 3,04 & 78,06        & 74,39         & 51,67   & 11,83 & 76,11        & 66,74         & 53,49   & 111,99 \\ \hline
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
\caption{Resultados Totales}
\label{my-label}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-13}
                            & \multicolumn{4}{c|}{Wdbc}                      & \multicolumn{4}{c|}{Movement\_Libras}           & \multicolumn{4}{c|}{Arrhythmia}                  \\ \cline{2-13} 
                            & \%\_clas\_in & \%\_clas\_out & \%\_red & T     & \%\_clas\_in & \%\_clas\_out & \%\_red & T      & \%\_clas\_in & \%\_clas\_out & \%\_red & T       \\ \hline
\multicolumn{1}{|l|}{3-NN}  & 96,417       & 95,922        & 0       & 0     & 71,334       & 73,11         & 0       & 0      & 62,95        & 63,575        & 0       & 0       \\ \hline
\multicolumn{1}{|l|}{SFS}   & 97,05        & 94,94         & 86      & 0,206 & 75,054       & 67,445        & 91,443  & 0,889  & 80,62        & 71,285        & 97,048  & 3,141   \\ \hline
\multicolumn{1}{|l|}{BMB}   & 98,35        & 95,43         & 48,33   & 3,42  & 77,67        & 73,00         & 55,22   & 13,00  & 72,95        & 64,87         & 51,73   & 141,21  \\ \hline
\multicolumn{1}{|l|}{GRASP} & 98,417       & 94,691        & 59,667  & 9,964 & 80,223       & 72,945        & 84,112  & 34,521 & 83,729       & 72,592        & 93,166  & 113,963 \\ \hline
\multicolumn{1}{|l|}{ILS}   & 98,527       & 95,539        & 46,329  & 3,037 & 78,055       & 74,389        & 51,665  & 11,832 & 76,11        & 66,738        & 53,491  & 111,994 \\ \hline
\end{tabular}}
\end{table}

\subsection{\color[rgb]{0.0,0.0,0.51}Análisis de resultados}


\begin{figure}[H]
\centering
\includegraphics[width=130mm]{tasa_train.png}
\end{figure}

Como podemos ver la tasa de acierto sobre Wdbc es la más elevada con respecto a las otras bases, además las tasas de acierto de todos lo algoritmos sobre esta base son muy parecidas, esto se debe a que, como dijo el profesor, esta base de datos es muy pequeña, es decir, el espacio de búsqueda es muy pequeño ($2^{30}$) y hay muy poca tasa de mejora entre una solución y otra.\\

Las tasas de acierto tanto para arritmia como para libras son muy parecidas como podemos ver en la gráfica para cada algoritmo. Es importante señalar cómo la tasa de acierto de los algoritmos usados en mejor que la tasa que da el 3NN original, es decir, usando todos los datos de la muestra. Con lo cual aquí se pone de manifiesto cómo el hecho de reducir el número de características a considerar no merma la correcta clasificación de los datos, al menos dentro de la muestra ya veremos qué sucede fuera. De hecho reduciendo el número de características no sólo reducimos el cómputo necesario sino también el posible ruido que pueden tener algunos datos.\\

Es curioso ver cómo el algoritmo greedy produce los mejores resultados en la base de datos con un espacio de búsqueda mayor, de hecho produce mejores resultados en esa base de datos que en la de libras que tiene un espacio menor, probablemente por la naturaleza de los datos, es decir, cómo de distintas sean las muestras que componen las distintas clases entre sí. En mi opinión el hecho de que el SFS produzca mejores resultados que la mayoría en el espacio de búsqueda más grande se debe a que al fin y al cabo el greedy en cada momento va construyendo la mejor solución que puede mientras que en cambio el resto de métodos depende más de cómo de buena sea la exploración que realicen. En cambio el GRASP sí que mejora lo que hace el SFS ya que GRASP no hace un sólo greedy sino que hace 25 optimizando las soluciones que encuentra mediante una búsqueda local, esto le da esa mejora que presenta. Como sabemos el greedy que usa GRASP tiene una componente aleatoria pero aún así esa aleatoriedad se da entre unas soluciones que superan un cierto umbral de calidad, con lo que el hecho de que el GRASP se comporte mejor que el SFS está justificado.\\

En cambio sí podemos ver cómo para el espacio más reducido de libras (también se ve en wdbc) sí que los algoritmos que hacen una búsqueda más extensa en el terreno, dan resultados mejores que SFS ya que exploran más soluciones y por lo tanto tienen mayor posibilidad de encontrar una mejor.\\

Si ahora comparamos los tres algoritmos que son nuevos en esta práctica vemos como la BMB es la que peor resultados obtiene, al fin y al cabo la búsqueda que realiza es la más aleatoria de todas y aunque esto le da una componente interesante de diversificación no hace que encuentre soluciones mejores a los otros algoritmos que sí que tienen diversificación pero con una cierta tendencia a la intesificación.\\

Entre GRASP e ILS, ILS tiene un mejor comportamiento en wdbc pero con un margen muy pequeño, ya hemos dicho que en esta base la diferencia entre soluciones es muy pequeña. En cambio en las otras dos bases de datos es el GRASP el que se impone. En mi opinión esto se debe a que GRASP aunque diversifica por su componente aleatoria no deja de intensificar en cada una de sus iteraciones una solución que ya es bastante buena a priori pues es construida mediante un procedimiento greedy al fin y al cabo. En cambio ILS parte en inicio de la solución de una BL que parte de un punto aleatorio que luego irá mutando pero esta mutación es de como mucho un 10\% de las características, con lo cual si la solución que ha encontrado es mala no da grandes saltos en sólo 25 iteraciones como para explorar en gran medida el espacio de búsqueda y encontrar una solución mejor que GRASP mediante su estrategia de diversificación.\\

\begin{figure}[H]
\centering
\includegraphics[width=130mm]{tasa_test.png}
\end{figure}

Veamos ahora cómo se comportan los algoritmos con respecto a la clasificación que producen sus características seleccionadas para los datos fuera de la muestra de entrenamiento. Lo primero que señalamos es que en la amplia mayoría de los casos la tasa de acierto es mayor dentro de la muestra que fuera, pero hay algunos casos donde es al revés. Esto se debe a que es probable que haya muestras de entrenamiento de una clase más cercanas a otra muestra pese a no pertenecer a la clase de dicha muestra. Con lo cual se puede dar el caso de que la "votación" sea mala en la muestra y en cambio la muestra fuera la distribución de puntos sea más favorable a una correcta clasificación de los datos. Por lo tanto no tiene que extrañarnos demasiado esto. Quizás si usásemos un 1NN sí que sería más probable que la muestra más cercana a otra perteneciera a la misma clase que ella (decimos la más cercana y no que en el 1NN hubiésemos obtenido una tasa de acierto del 100\% dentro de la muestra puesto que usamos para evitar esto el Leave One Out).\\

Nuevamente el comportamiento para Wdbc es muy similar para todos los algoritmos, lo que era esperable por el razonamiento que hemos dado antes. Lo que sí podemos ver es cómo en esta ocasión el 3NN está en los primeros puestos para las bases de datos de wdbc y libras además de que hay otros algoritmos que también cambian de posición. Esto nos informa de dos cosas: la primera es del eterno problema que hay en el aprendizaje automático y es la capacidad de generalización, el no poder asegurar que si una solución es buena dentro de la muestra de entrenamiento vaya a ser fuera. En segundo lugar vemos cómo lo algoritmos son fuertemente dependientes de los datos, el ruido o simplemente los datos tomados afectas al rendimiento de las soluciones encontradas.\\

Esto último se pone muy en relieve cuando vemos cómo el GRASP es de los peores algoritmos en las bases de datos de wdbc y libras, en mi oponión se ve más afectada en estas bases porque puede ser que los datos tengan ruido o no informen lo suficiente y al ser el algoritmo que mayor intesificación realiza puede que sea el que más sufre de sobreajuste. Ahora bien, en la base de datos de arritmia es el mejor algoritmo indicando que o bien los datos de esta base dan mucha información o que el espacio de búsqueda queda grande para el resto de algoritmos que dependen más de la exploración realizada. De hecho el otro algoritmo que peor funciona en libras y wdbc es precisamente el SFS que también puede verse afectado por el sobreajuste al no tener mecanismos de diversificación.\\

El hecho de que el 3NN con toda la información no mejore a todos los algoritmos puede venir de que no sólo haya características irrelevantes sino que, como hemos señalado anteriormente, haya datos cuya medición haya sido errónea y que simplemente lo que nos estén haciendo sea empeorar el resultado.\\

\begin{figure}[H]
\centering
\includegraphics[width=130mm]{reduccion.png}
\end{figure}

Evidentemente la tasa de reducción para el 3NN es 0 pues tomamos todas las características al no tener otro criterio. Ahora el que produce mayor reducción es el greedy lo que es lógico por la forma en la que trabaja, agregando progresivamente propiedades hasta no encontrar mejora ninguna.\\

Como era de esperar el siguiente que tiene una mayor tasa de reducción es el GRASP el cual al fin y al cabo tiene un comportamiento greedy y como las soluciones que produce el greedy aleatorio (que es el que genera mayor reducción) ya son los suficientemente buenas la búsqueda local no le "añade" muchas más características a la solución de la que parte.

Las tasas de reducción para el resto la BMB y el ILS son muy similares entre sí, si lo pensamos dado que nuestra función de coste sólo tiene en cuenta la tasa media de acierto en la muestra de entrenamiento (no tenemos en cuenta también la reducción que consigue) es lógico que esto suceda así, o al menos que no haya un algoritmo que claramente produzca mejores resultados que otro. La exploración que se hace es de algún modo u otro aleatoria con lo que dependiendo del azar será un algoritmo o el otro el que obtenga una solución con un menor número de características.\\


\begin{figure}[H]
\centering
\includegraphics[width=130mm]{tiempo.png}
\end{figure}


Evidentemente el 3NN tarda 0 segundo puesto que su tiempo de búsqueda de solución es 0 pues simplemente devuelve un solución con todos los valores a True, es decir, escoge todas las características sin tener ningún criterio. Y observamos también que todos los algoritmos si los analizamos individualmente tardan más cuanto mayor es el espacio de búsqueda (ahora el LOO no tiene tanto impacto y con lo cual el número de muestras por partición no resulta tan decisivo como sí ocurría en la práctica anterior cuando se tardaba más en libras que en arritmia para algunos algoritmos).\\

El GRASP es el algoritmo que tarda salvo en arritmia donde sólo le supero la BMB. Esto se debe a que es el que realiza las operaciones más costosas al hacer por cada ejecución un greedy y a continuación una búsqueda local. Sin embargo en la base de datos con un espacio de búsqueda mayor es la BMB la que más tarda, probablemente esto se deba a que pese a que la generación de la solución de la que parte la BL en GRASP es más costosa en tiempo sí que hace que se parta de una buena solución y que por tanto la BL tarde menos en no poder mejorar la solución de la que parte, algo que es muy importante cuando el espacio de búsqueda es tan grande.

De los tres algoritmos que hemos implementado en esta práctica el ILS es el más rápido esto se debe a la combinación de dos factores: las operaciones que realiza son poco costosas con respecto a las de los otros dos algoritmos y además tiene una componente de intesificación al mutar, un 10\% solamente, la mejor solución encontrada hasta el momento, haciendo, como hemos analizado anteriorment, que la BL tarde menos. De hecho vemos como en bases de datos con un espacio de búsqueda menor ILS y BMB tienen tiempos similares ya que esta intensificación no tiene tanta importancia.\\

Por último el SFS tarda muy poco con respecto al resto de algoritmos ya que se trata únicamente de una búsqueda greedy y no 25 como hace GRASP ni 25 búsquedas locales como hacen los otros algoritmos.\\

\newpage
\section{\color[rgb]{0.0,0.0,0.21}Bibliografía}

\begin{itemize}
\item Documentación del módulo scikit-learn: \url{http://scikit-learn.org/stable/documentation.html}
\item Documentación de Numpy: \url{http://docs.scipy.org/doc/}
\end{itemize}
\end{document}